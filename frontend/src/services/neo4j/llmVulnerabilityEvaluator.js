import { apiClient } from './axiosConfig';

export class LLMVulnerabilityEvaluator {
  constructor(driver) {
    this.driver = driver;
    this.ollamaApiUrl = import.meta.env.VITE_OLLAMA_API_URL || 'http://localhost:11434';
    this.ollamaModel = import.meta.env.VITE_OLLAMA_MODEL || 'llama2:latest';
    
    // Blocklisted extensions and paths for files to exclude from analysis
    this.blockedExtensions = [
      '.css', '.lock', '.md', '.min.js', '.scss', '.txt', '.rst', 
      '.json', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', 
      '.eot', '.ttf', '.woff', '.woff2', '.map', '.yaml', '.yml'
    ];
    
    this.maxFileSize = 200000; // 200,000 characters limit for files
  }

  /**
   * Evaluates a repository revision for vulnerabilities using LLM
   * @param {string} repoId - The ID of the repository to evaluate
   * @param {string} versionId - The specific version/revision to evaluate
   * @returns {Promise<object>} - The evaluation results
   */
  async evaluateRevision(repoId, versionId) {
    try {
      // 1. Fetch the code files for this revision
      const codeFiles = await this.getCodeFilesForRevision(repoId, versionId);
      
      // 2. Get relevant CVEs/CWEs for this codebase
      const vulnerabilities = await this.getRelevantVulnerabilities(repoId, versionId);
      
      // 3. For each vulnerability, run the LLM evaluation
      const evaluationResults = [];
      
      for (const vuln of vulnerabilities) {
        const result = await this.evaluateVulnerabilityWithLLM(codeFiles, vuln);
        evaluationResults.push(result);
        
        // Store the evaluation result in the database
        await this.storeEvaluationResult(repoId, versionId, vuln.id, result);
      }
      
      return {
        repositoryId: repoId,
        versionId: versionId,
        evaluationResults
      };
    } catch (error) {
      console.error('Error evaluating revision:', error);
      throw error;
    }
  }

  /**
   * Fetches code files for a specific repository revision
   * Applies filtering rules based on requirements
   */
  async getCodeFilesForRevision(repoId, versionId) {
    const session = this.driver.session();
    try {
      const result = await session.run(`
        MATCH (r:Repository {id: $repoId})-[:HAS_VERSION]->(v:Version {id: $versionId})-[:CONTAINS]->(f:File)
        RETURN f.path AS path, f.content AS content, f.mime_type AS mimeType, f.size AS size
      `, {
        repoId,
        versionId
      });
      
      // Apply filtering rules:
      // 1. Exclude files with blocklisted extensions
      // 2. Exclude files in paths starting with "."
      // 3. Exclude files larger than 200,000 characters
      // 4. Exclude files with non-text MIME types
      const filteredFiles = result.records
        .map(record => ({
          path: record.get('path'),
          content: record.get('content'),
          mimeType: record.get('mimeType'),
          size: record.get('size')
        }))
        .filter(file => {
          // Check file extension
          const extension = file.path.substring(file.path.lastIndexOf('.'));
          if (this.blockedExtensions.includes(extension)) return false;
          
          // Check path for dot files/directories
          const pathParts = file.path.split('/');
          if (pathParts.some(part => part.startsWith('.'))) return false;
          
          // Check file size
          if (file.content && file.content.length > this.maxFileSize) return false;
          
          // Check MIME type
          if (file.mimeType && !file.mimeType.startsWith('text/')) return false;
          
          return true;
        });
      
      return filteredFiles;
    } finally {
      await session.close();
    }
  }

  /**
   * Gets relevant vulnerabilities for a repository
   */
  async getRelevantVulnerabilities(repoId, versionId) {
    const session = this.driver.session();
    try {
      const result = await session.run(`
        MATCH (r:Repository {id: $repoId})-[:HAS_VERSION]->(v:Version {id: $versionId})
        MATCH (v)-[:USES_PACKAGE]->(p:Package)<-[:AFFECTS]-(vuln:Vulnerability)
        OPTIONAL MATCH (c:CVE)-[:IDENTIFIED_AS]->(vuln)
        RETURN vuln, collect(DISTINCT c.id) AS cveIds
      `, {
        repoId,
        versionId
      });
      
      return result.records.map(record => {
        const vuln = record.get('vuln').properties;
        return {
          ...vuln,
          cveIds: record.get('cveIds')
        };
      });
    } finally {
      await session.close();
    }
  }

  /**
   * Evaluates a vulnerability against codebase using LLM
   */
  async evaluateVulnerabilityWithLLM(codeFiles, vulnerability) {
    try {
      // Construct the prompt for the LLM
      const prompt = this.constructLLMPrompt(codeFiles, vulnerability);
      
      // Call Ollama API
      const response = await fetch(`${this.ollamaApiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.ollamaModel,
          prompt: prompt,
          stream: false
        })
      });
      
      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.status}`);
      }
      
      const data = await response.json();
      
      // Parse the response to extract structured data
      return this.parseLLMResponse(data.response, vulnerability);
    } catch (error) {
      console.error('Error calling LLM:', error);
      return {
        vulnerabilityId: vulnerability.id,
        headline: 'Error during evaluation',
        analysis: `Failed to evaluate: ${error.message}`,
        cve: vulnerability.cveIds?.[0] || 'Unknown',
        concernedFunctions: [],
        concernedFiles: [],
        classification: 'not promising',
        error: error.message
      };
    }
  }

  /**
   * Constructs a prompt for the LLM based on the codebase and vulnerability
   */
  constructLLMPrompt(codeFiles, vulnerability) {
    // Include only the first N files to avoid token limits
    const MAX_FILES = 10;
    const selectedFiles = codeFiles.slice(0, MAX_FILES);
    
    // Format the code snippets
    const codeSnippets = selectedFiles.map(file => 
      `File: ${file.path}\n\`\`\`\n${file.content}\n\`\`\``
    ).join('\n\n');
    
    // Format CVE information
    const cveInfo = vulnerability.cveIds && vulnerability.cveIds.length > 0 
      ? vulnerability.cveIds.join(', ') 
      : 'No specific CVE';
    
    // Prepare the prompt
    return `You are a security expert analyzing code for potential vulnerabilities.
    
VULNERABILITY DETAILS:
ID: ${vulnerability.id}
Summary: ${vulnerability.summary || 'Not provided'}
Details: ${vulnerability.details || 'Not provided'}
CVE(s): ${cveInfo}

CODE TO ANALYZE:
${codeSnippets}

INSTRUCTIONS:
1. Analyze the code for the specified vulnerability.
2. Provide a concise headline for the vulnerability.
3. Write a detailed analysis of how the vulnerability affects this codebase.
4. Identify the most relevant CVE type for this vulnerability.
5. List the most concerned functions in the code (those most likely affected).
6. List the most concerned filenames in the codebase.
7. Classify the likelihood of this vulnerability being present as one of: "very promising", "slightly promising", or "not promising".

Format your response exactly like this:
HEADLINE: [Vulnerability headline]
ANALYSIS: [Detailed analysis]
RELEVANT_CVE: [Most relevant CVE type]
CONCERNED_FUNCTIONS: [List of functions, comma-separated]
CONCERNED_FILES: [List of filenames, comma-separated]
CLASSIFICATION: [very promising|slightly promising|not promising]

Be precise and concise. Base your classification strictly on the evidence in the code.`;
  }

  /**
   * Parses the LLM response into structured data
   */
  parseLLMResponse(response, vulnerability) {
    const result = {
      vulnerabilityId: vulnerability.id,
      headline: '',
      analysis: '',
      cve: '',
      concernedFunctions: [],
      concernedFiles: [],
      classification: 'not promising' // default
    };
    
    // Extract the headline
    const headlineMatch = response.match(/HEADLINE:\s*(.+?)(?=\n|ANALYSIS:)/s);
    if (headlineMatch) {
      result.headline = headlineMatch[1].trim();
    }
    
    // Extract the analysis
    const analysisMatch = response.match(/ANALYSIS:\s*(.+?)(?=\n|RELEVANT_CVE:)/s);
    if (analysisMatch) {
      result.analysis = analysisMatch[1].trim();
    }
    
    // Extract the CVE
    const cveMatch = response.match(/RELEVANT_CVE:\s*(.+?)(?=\n|CONCERNED_FUNCTIONS:)/s);
    if (cveMatch) {
      result.cve = cveMatch[1].trim();
    }
    
    // Extract concerned functions
    const functionsMatch = response.match(/CONCERNED_FUNCTIONS:\s*(.+?)(?=\n|CONCERNED_FILES:)/s);
    if (functionsMatch) {
      result.concernedFunctions = functionsMatch[1]
        .split(',')
        .map(func => func.trim())
        .filter(Boolean);
    }
    
    // Extract concerned files
    const filesMatch = response.match(/CONCERNED_FILES:\s*(.+?)(?=\n|CLASSIFICATION:)/s);
    if (filesMatch) {
      result.concernedFiles = filesMatch[1]
        .split(',')
        .map(file => file.trim())
        .filter(Boolean);
    }
    
    // Extract classification
    const classMatch = response.match(/CLASSIFICATION:\s*(.+?)(?=\n|$)/s);
    if (classMatch) {
      const classification = classMatch[1].trim().toLowerCase();
      if (["very promising", "slightly promising", "not promising"].includes(classification)) {
        result.classification = classification;
      }
    }
    
    return result;
  }

  /**
   * Stores evaluation results in the database
   */
  async storeEvaluationResult(repoId, versionId, vulnId, evaluationResult) {
    const session = this.driver.session();
    try {
      await session.run(`
        MATCH (r:Repository {id: $repoId})-[:HAS_VERSION]->(v:Version {id: $versionId})
        MATCH (vuln:Vulnerability {id: $vulnId})
        MERGE (e:LLMEvaluation {
          id: $repoId + '_' + $versionId + '_' + $vulnId
        })
        SET e.headline = $headline,
            e.analysis = $analysis,
            e.cve = $cve,
            e.concernedFunctions = $concernedFunctions,
            e.concernedFiles = $concernedFiles,
            e.classification = $classification,
            e.evaluatedAt = datetime()
        MERGE (v)-[:HAS_EVALUATION]->(e)
        MERGE (e)-[:EVALUATES]->(vuln)
        RETURN e
      `, {
        repoId,
        versionId,
        vulnId,
        headline: evaluationResult.headline,
        analysis: evaluationResult.analysis,
        cve: evaluationResult.cve,
        concernedFunctions: evaluationResult.concernedFunctions,
        concernedFiles: evaluationResult.concernedFiles,
        classification: evaluationResult.classification
      });
    } finally {
      await session.close();
    }
  }
  
  /**
   * Calculate evaluation metrics for LLM performance (bonus feature)
   */
  async calculateEvaluationMetrics() {
    const session = this.driver.session();
    try {
      // Get all evaluations
      const result = await session.run(`
        MATCH (e:LLMEvaluation)
        RETURN e.classification as classification, count(*) as count
      `);
      
      // Calculate distribution of classifications
      const metrics = {
        totalEvaluations: 0,
        classificationDistribution: {
          'very promising': 0,
          'slightly promising': 0,
          'not promising': 0
        },
        precision: 0,
        recall: 0,
        f1Score: 0
      };
      
      result.records.forEach(record => {
        const classification = record.get('classification');
        const count = record.get('count').toInt();
        
        if (metrics.classificationDistribution.hasOwnProperty(classification)) {
          metrics.classificationDistribution[classification] = count;
          metrics.totalEvaluations += count;
        }
      });
      
      // For precision, recall, and F1-score, we would need ground truth data
      // Since that's not available, we'll return the basic metrics
      
      return metrics;
    } finally {
      await session.close();
    }
  }
}

export default LLMVulnerabilityEvaluator;
